# Knowledge Distillation Report: V4 Lite from V3 LSTM

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

### ‚úÖ –£—Å–ø–µ—Ö! Student –ø—Ä–µ–≤–∑–æ—à—ë–ª Teacher

| –ú–æ–¥–µ–ª—å | MCC | Accuracy | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã |
|--------|-----|----------|-----------|
| **V3 LSTM (Teacher)** | 0.1224 | 62.2% | 53,122 |
| **V4 Lite Distilled (Student)** | **0.1495** | 57.4% | 83,202 |
| V4 Full Transformer | 0.00 | 33% | 1,082,118 |

### üéØ –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

```
üìä Test Results:
   Accuracy: 0.5742
   F1 Macro: 0.5666
   MCC:      0.1495

üìã Classification Report:
              precision    recall  f1-score   support

        DOWN       0.45      0.59      0.51      1140
          UP       0.70      0.56      0.62      1901

üéì Teacher MCC: 0.1224
üéØ Student MCC: 0.1495
‚úÖ Student BEATS teacher!
```

## –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è

### Knowledge Distillation

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ø–æ–¥—Ö–æ–¥ knowledge distillation, –≥–¥–µ:
- **Teacher**: V3 LSTM (direction_lstm_hybrid_XAUUSD.pt)
- **Student**: V4 Lite Transformer

### Loss Function

```python
Loss = Œ± * CrossEntropy(student, hard_labels) + (1-Œ±) * T¬≤ * KL_Div(student/T, teacher/T)
```

### –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |
|----------|----------|
| Alpha (hard label weight) | 0.8 |
| Temperature | 1.0 |
| Learning Rate | 0.0002 |
| Epochs | 18 (early stopping) |
| Optimizer | Adam |

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ V4 Lite

```python
GoldenBreezeLite(
    input_dim=15,           # Engineered features (v3-style)
    d_model=64,             # Transformer dimension
    n_heads=4,              # Attention heads
    n_encoder_layers=2,     # Encoder layers
    strategy_dim=33,        # Strategy signals
    smc_static_dim=8,       # SMC static features
    output_dim=2,           # DOWN/UP
)
```

### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

1. **15 V3-style features:**
   - close, returns, log_returns
   - sma_fast, sma_slow, sma_ratio
   - atr, atr_norm, rsi, bb_position
   - volume_ratio, momentum, volatility_ratio
   - high_low_ratio, close_position

2. **33 Strategy signals:**
   - Trend indicators
   - Momentum oscillators
   - Volume analysis
   - Multi-timeframe confluence

3. **8 SMC static features:**
   - Price position
   - Structure analysis

## –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

### Teacher Predictions Distribution
```
Teacher predictions: DOWN=33,200, UP=1,658
```
Teacher —Å–∏–ª—å–Ω–æ —Å–º–µ—â—ë–Ω –∫ DOWN (95% vs 5%)

### Weighted Sampling
```
Class weights: DOWN=1.43, UP=0.77
```
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è WeightedRandomSampler –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏.

### Training Progress
```
Epoch  18/100 | MCC: 0.0660 (val)
...
Test MCC: 0.1495 (best)
```

## –§–∞–π–ª—ã

| –§–∞–π–ª | –û–ø–∏—Å–∞–Ω–∏–µ |
|------|----------|
| `models/v4_lite_distilled.pt` | –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å V4 Lite |
| `models/v4_lite_history.json` | –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è |
| `aimodule/training/train_v4_lite_distill.py` | –°–∫—Ä–∏–ø—Ç distillation |
| `aimodule/models/v4_transformer/model_lite.py` | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ V4 Lite |

## –í—ã–≤–æ–¥—ã

1. **Knowledge Distillation —Ä–∞–±–æ—Ç–∞–µ—Ç**: Student (V4 Lite) —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∏–ª—Å—è –æ—Ç Teacher (V3 LSTM)

2. **–ü—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ Teacher**: MCC —É–ª—É—á—à–∏–ª—Å—è —Å 0.12 –¥–æ **0.15** (+22%)

3. **V4 Lite vs V4 Full**: 
   - V4 Full (1M params) ‚Üí MCC=0.00 (collapsed)
   - V4 Lite (83K params) ‚Üí MCC=0.15 (working)

4. **–ö–ª—é—á –∫ —É—Å–ø–µ—Ö—É**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ engineered features –≤–º–µ—Å—Ç–æ raw OHLCV
   - –í—ã—Å–æ–∫–∏–π alpha (0.8) ‚Äî –±–æ–ª—å—à–µ hard labels
   - –ù–∏–∑–∫–∏–π temperature (1.0) ‚Äî —á—ë—Ç–∫–∏–µ predictions

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `v4_lite_distilled.pt` –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
2. –ü—Ä–∏ –¥–∞–ª—å–Ω–µ–π—à–µ–º –æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å alpha=0.8, T=1.0
3. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å progressive distillation –¥–ª—è –µ—â—ë –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

---

*Generated: 2024-12-04*
*Best MCC: 0.1495*
