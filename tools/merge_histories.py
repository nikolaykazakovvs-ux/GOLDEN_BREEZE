"""
PHASE 6: Big Data Integration - Merge Historical Data
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç H1 (6 –ª–µ—Ç) —Å —Ç–µ–∫—É—â–µ–π M5 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ–≥–∞-–¥–∞—Ç–∞—Å–µ—Ç–∞
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime, timedelta

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)
logger = logging.getLogger(__name__)

DATA_DIR = Path('data/raw/XAUUSD')
OUTPUT_DIR = Path('data/prepared')

class HistoryMerger:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å H1 (6 –ª–µ—Ç) –∫–∞–∫ –æ—Å–Ω–æ–≤—É, —Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –≤ M5
    """
    
    def __init__(self):
        self.h1_data = None
        self.m5_data = None
        self.d1_data = None
    
    def load_h1(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å H1 –¥–∞–Ω–Ω—ã–µ (6 –ª–µ—Ç)"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ H1 –¥–∞–Ω–Ω—ã—Ö (2019-2025)...")
        
        df = pd.read_csv(DATA_DIR / 'H1.csv')
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏
        if 'timestamp' in df.columns:
            time_col = 'timestamp'
        elif 'date' in df.columns:
            time_col = 'date'
        else:
            time_col = df.columns[0]
        
        df['timestamp'] = pd.to_datetime(df[time_col])
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏
        expected_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        for col in expected_cols[1:]:
            if col not in df.columns:
                # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –ø–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º –∏–º–µ–Ω–∞–º
                alt_names = {
                    'open': ['Open', 'OPEN', 'o'],
                    'high': ['High', 'HIGH', 'h'],
                    'low': ['Low', 'LOW', 'l'],
                    'close': ['Close', 'CLOSE', 'c'],
                    'volume': ['Volume', 'VOLUME', 'vol', 'Vol']
                }
                for alt in alt_names.get(col, []):
                    if alt in df.columns:
                        df[col] = df[alt]
                        break
        
        # –í—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–µ—Å–ª–∏ volume –Ω–µ—Ç, —Å–æ–∑–¥–∞—Ç—å –Ω—É–ª–µ–≤–æ–π)
        if 'volume' not in df.columns:
            df['volume'] = 0
        
        df = df[expected_cols]
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        self.h1_data = df
        logger.info(f"‚úÖ H1: {len(df)} –±–∞—Ä–æ–≤, {df['timestamp'].min()} - {df['timestamp'].max()}")
        return df
    
    def load_m5(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—É—â–∏–µ M5 –¥–∞–Ω–Ω—ã–µ (2024-2025)"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ M5 –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—É—â–∏–µ)...")
        
        df = pd.read_csv(DATA_DIR / 'M5.csv')
        
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ H1
        if 'timestamp' in df.columns:
            time_col = 'timestamp'
        elif 'date' in df.columns:
            time_col = 'date'
        else:
            time_col = df.columns[0]
        
        df['timestamp'] = pd.to_datetime(df[time_col])
        
        expected_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        for col in expected_cols[1:]:
            if col not in df.columns:
                alt_names = {
                    'open': ['Open', 'OPEN', 'o'],
                    'high': ['High', 'HIGH', 'h'],
                    'low': ['Low', 'LOW', 'l'],
                    'close': ['Close', 'CLOSE', 'c'],
                    'volume': ['Volume', 'VOLUME', 'vol', 'Vol']
                }
                for alt in alt_names.get(col, []):
                    if alt in df.columns:
                        df[col] = df[alt]
                        break
        
        # –í—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–µ—Å–ª–∏ volume –Ω–µ—Ç, —Å–æ–∑–¥–∞—Ç—å –Ω—É–ª–µ–≤–æ–π)
        if 'volume' not in df.columns:
            df['volume'] = 0
        
        df = df[expected_cols]
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        self.m5_data = df
        logger.info(f"‚úÖ M5: {len(df)} –±–∞—Ä–æ–≤, {df['timestamp'].min()} - {df['timestamp'].max()}")
        return df
    
    def resample_h1_to_m5(self):
        """
        –†–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å H1 –≤ M5 (—Ñ–µ–π–∫–æ–≤—ã–π, –Ω–æ –ª—É—á—à–µ —á–µ–º –Ω–∏—á–µ–≥–æ)
        –†–∞–∑–±–∏–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π H1 –±–∞—Ä –Ω–∞ 12 M5 –±–∞—Ä–æ–≤ —Å –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–µ–π
        """
        logger.info("üîÑ –†–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ H1 ‚Üí M5 (–∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è)...")
        
        if self.h1_data is None:
            raise ValueError("H1 –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        
        m5_synthetic = []
        
        for idx, row in self.h1_data.iterrows():
            h1_time = row['timestamp']
            h1_open = row['open']
            h1_high = row['high']
            h1_low = row['low']
            h1_close = row['close']
            h1_volume = row['volume']
            
            # –°–æ–∑–¥–∞—Ç—å 12 M5 –±–∞—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ H1
            for i in range(12):
                m5_time = h1_time + timedelta(minutes=5*i)
                
                # –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è —Ü–µ–Ω
                progress = i / 11.0 if i < 11 else 1.0
                m5_open = h1_open + (h1_close - h1_open) * (i / 12.0)
                m5_close = h1_open + (h1_close - h1_open) * ((i+1) / 12.0)
                
                # High/Low —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —à—É–º–æ–º
                m5_high = max(m5_open, m5_close) + abs(h1_high - h1_close) * 0.1
                m5_low = min(m5_open, m5_close) - abs(h1_close - h1_low) * 0.1
                
                # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ High/Low –Ω–µ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã H1
                m5_high = min(m5_high, h1_high)
                m5_low = max(m5_low, h1_low)
                
                m5_volume = h1_volume / 12  # –†–∞–∑–¥–µ–ª–∏—Ç—å –æ–±—ä—ë–º
                
                m5_synthetic.append({
                    'timestamp': m5_time,
                    'open': m5_open,
                    'high': m5_high,
                    'low': m5_low,
                    'close': m5_close,
                    'volume': m5_volume
                })
        
        df_m5_synthetic = pd.DataFrame(m5_synthetic)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(df_m5_synthetic)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö M5 –±–∞—Ä–æ–≤")
        
        return df_m5_synthetic
    
    def merge_datasets(self):
        """
        –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π M5 (–∏–∑ H1) —Å —Ä–µ–∞–ª—å–Ω–æ–π M5
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –°–∏–Ω—Ç–µ—Ç–∏–∫–∞ –¥–ª—è 2019-2024, —Ä–µ–∞–ª—å–Ω–∞—è –¥–ª—è 2024-2025
        """
        logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
        
        # –†–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å H1 ‚Üí M5
        m5_synthetic = self.resample_h1_to_m5()
        
        # –ù–∞–π—Ç–∏ —Ç–æ—á–∫—É –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        if self.m5_data is not None:
            cutoff_date = self.m5_data['timestamp'].min()
            logger.info(f"üìç –¢–æ—á–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è: {cutoff_date}")
            
            # –í–∑—è—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏–∫—É –¥–æ cutoff, —Ä–µ–∞–ª—å–Ω—É—é –ø–æ—Å–ª–µ
            m5_old = m5_synthetic[m5_synthetic['timestamp'] < cutoff_date]
            m5_new = self.m5_data[self.m5_data['timestamp'] >= cutoff_date]
            
            combined = pd.concat([m5_old, m5_new], ignore_index=True)
        else:
            # –ï—Å–ª–∏ M5 –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏–∫—É
            combined = m5_synthetic
        
        combined = combined.sort_values('timestamp').reset_index(drop=True)
        
        logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(combined)} M5 –±–∞—Ä–æ–≤")
        logger.info(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {combined['timestamp'].min()} - {combined['timestamp'].max()}")
        logger.info(f"   üïê –î–Ω–µ–π: {(combined['timestamp'].max() - combined['timestamp'].min()).days}")
        
        return combined
    
    def validate_data(self, df):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ OHLC
        invalid_ohlc = df[df['high'] < df['low']]
        if len(invalid_ohlc) > 0:
            issues.append(f"‚ùå {len(invalid_ohlc)} –±–∞—Ä–æ–≤ —Å High < Low")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        time_diffs = df['timestamp'].diff()
        expected_diff = pd.Timedelta(minutes=5)
        gaps = time_diffs[time_diffs > expected_diff * 2]  # –ü—Ä–æ–ø—É—Å–∫ >10 –º–∏–Ω
        
        if len(gaps) > 100:  # –í—ã—Ö–æ–¥–Ω—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
            logger.warning(f"‚ö†Ô∏è {len(gaps)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–≤—ã—Ö–æ–¥–Ω—ã–µ/–ø—Ä–∞–∑–¥–Ω–∏–∫–∏)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        dupes = df[df['timestamp'].duplicated()]
        if len(dupes) > 0:
            issues.append(f"‚ùå {len(dupes)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
        
        if len(issues) == 0:
            logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞")
        else:
            for issue in issues:
                logger.error(issue)
        
        return len(issues) == 0
    
    def save_merged_data(self, df, filename='M5_merged_2019_2025.csv'):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        output_path = OUTPUT_DIR / filename
        df.to_csv(output_path, index=False)
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
        logger.info(f"   üìä –†–∞–∑–º–µ—Ä: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
        
        # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞—Ç—å –∫–æ–ø–∏—é –≤ raw –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        (DATA_DIR / 'M5_6year.csv').write_text(output_path.read_text())
        logger.info(f"üíæ –ö–æ–ø–∏—è: {DATA_DIR / 'M5_6year.csv'}")
        
        return output_path
    
    def run(self):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å"""
        logger.info("\n" + "="*70)
        logger.info("üöÄ PHASE 6: BIG DATA INTEGRATION")
        logger.info("="*70)
        
        # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞
        self.load_h1()
        self.load_m5()
        
        # –®–∞–≥ 2: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        merged_df = self.merge_datasets()
        
        # –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self.validate_data(merged_df):
            logger.error("‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ—à–ª–∞, –∏—Å–ø—Ä–∞–≤—å—Ç–µ –æ—à–∏–±–∫–∏")
            return None
        
        # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        output_path = self.save_merged_data(merged_df)
        
        logger.info("\n" + "="*70)
        logger.info("‚úÖ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("="*70)
        logger.info(f"\nüìÑ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
        logger.info(f"   python tools/precompute_v4_data.py")
        logger.info(f"\nüí° –ó–∞—Ç–µ–º –∑–∞–ø—É—Å—Ç–∏ Mega-Training:")
        logger.info(f"   python -m aimodule.training.train_v4_lstm --epochs 500 --batch-size 256\n")
        
        return output_path


if __name__ == '__main__':
    merger = HistoryMerger()
    merger.run()
