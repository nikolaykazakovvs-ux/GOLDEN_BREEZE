"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö XAU/USD
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)
logger = logging.getLogger(__name__)

DATA_DIR = Path('data/raw/XAUUSD')

class DataValidator:
    def __init__(self):
        self.results = {}
    
    def load_data(self, filepath):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å CSV —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        try:
            df = pd.read_csv(filepath)
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filepath} ({len(df)} —Å—Ç—Ä–æ–∫)")
            return df
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filepath}: {e}")
            return None
    
    def validate_ohlc(self, df, timeframe):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å OHLC"""
        issues = []
        
        if 'high' not in df.columns or 'low' not in df.columns:
            issues.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ high/low")
            return issues
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: High >= Low
        invalid = df[df['high'] < df['low']]
        if len(invalid) > 0:
            issues.append(f"‚ùå {len(invalid)} –±–∞—Ä–æ–≤ —Å High < Low (OHLC –Ω–∞—Ä—É—à–µ–Ω—ã)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: Open –∏ Close –º–µ–∂–¥—É High –∏ Low
        invalid_open = df[(df['open'] > df['high']) | (df['open'] < df['low'])]
        if len(invalid_open) > 0:
            issues.append(f"‚ö†Ô∏è {len(invalid_open)} –±–∞—Ä–æ–≤ –≥–¥–µ Open –≤–Ω–µ [Low, High]")
        
        if len(issues) == 0:
            issues.append(f"‚úÖ OHLC —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å: OK ({len(df)} –±–∞—Ä–æ–≤)")
        
        return issues
    
    def validate_timestamps(self, df, timeframe):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏"""
        issues = []
        
        if 'timestamp' not in df.columns and 'date' not in df.columns:
            issues.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞")
            return issues
        
        time_col = 'timestamp' if 'timestamp' in df.columns else 'date'
        
        try:
            df['dt'] = pd.to_datetime(df[time_col])
        except:
            issues.append(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –≤ –∫–æ–ª–æ–Ω–∫–µ '{time_col}'")
            return issues
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        dupes = df[df['dt'].duplicated()].shape[0]
        if dupes > 0:
            issues.append(f"‚ö†Ô∏è {dupes} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏
        df_sorted = df.sort_values('dt')
        dt_diffs = df_sorted['dt'].diff()
        
        expected_delta = pd.Timedelta(minutes=int(timeframe.rstrip('M')))
        gaps = dt_diffs[dt_diffs != expected_delta]
        
        if len(gaps) > 5:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –∏ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏
            issues.append(f"‚ö†Ô∏è {len(gaps)} –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–≤—ã—Ö–æ–¥–Ω—ã–µ/–ø—Ä–∞–∑–¥–Ω–∏–∫–∏/–ø—Ä–∞–∑–¥–Ω–∏–∫–∏ - –Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
        
        date_range = f"{df_sorted['dt'].min().date()} - {df_sorted['dt'].max().date()}"
        issues.append(f"‚úÖ –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {date_range}")
        
        return issues
    
    def validate_volume(self, df):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±—ä—ë–º—ã"""
        issues = []
        
        if 'volume' not in df.columns:
            issues.append("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ Volume –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return issues
        
        zero_vol = (df['volume'] == 0).sum()
        if zero_vol > 0:
            issues.append(f"‚ö†Ô∏è {zero_vol} –±–∞—Ä–æ–≤ —Å Volume=0 (—Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –∑–æ–ª–æ—Ç–µ)")
        
        neg_vol = (df['volume'] < 0).sum()
        if neg_vol > 0:
            issues.append(f"‚ùå {neg_vol} –±–∞—Ä–æ–≤ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –æ–±—ä—ë–º–æ–º")
        else:
            issues.append(f"‚úÖ Volume: –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è >= 0")
        
        return issues
    
    def check_data_sources(self):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("\n" + "="*70)
        logger.info("üìä –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–• XAU/USD")
        logger.info("="*70)
        
        sources = {
            'M5': 'M5.csv',
            'H1': 'H1.csv',
            'D1': 'investing_d1.csv'
        }
        
        for timeframe, filename in sources.items():
            filepath = DATA_DIR / filename
            
            logger.info(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ {timeframe} ({filename}):")
            logger.info("-" * 70)
            
            if not filepath.exists():
                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
                continue
            
            df = self.load_data(filepath)
            if df is None:
                continue
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            ohlc_issues = self.validate_ohlc(df, timeframe)
            time_issues = self.validate_timestamps(df, timeframe)
            vol_issues = self.validate_volume(df)
            
            for issue in ohlc_issues + time_issues + vol_issues:
                logger.info(f"  {issue}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info(f"\n  üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            logger.info(f"    ‚Ä¢ –†–∞–∑–º–µ—Ä: {len(df)} –±–∞—Ä–æ–≤")
            logger.info(f"    ‚Ä¢ –û—Ç–∫—Ä—ã—Ç–æ: {df['open'].min():.2f} - {df['open'].max():.2f}")
            logger.info(f"    ‚Ä¢ –û–±—ä—ë–º: {df['volume'].sum():,.0f} (—Å—Ä–µ–¥–Ω–µ–µ: {df['volume'].mean():.0f})")
            
            if 'close' in df.columns:
                returns = df['close'].pct_change().dropna()
                logger.info(f"    ‚Ä¢ –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: Œº={returns.mean()*100:.4f}% œÉ={returns.std()*100:.4f}%")
            
            self.results[timeframe] = {
                'file': filename,
                'rows': len(df),
                'issues': len([i for i in ohlc_issues + time_issues + vol_issues if '‚ùå' in i])
            }
        
        return self.results
    
    def print_summary(self):
        """–ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç"""
        logger.info("\n" + "="*70)
        logger.info("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢")
        logger.info("="*70)
        
        for tf, info in self.results.items():
            status = "‚úÖ" if info['issues'] == 0 else "‚ö†Ô∏è"
            logger.info(f"{status} {tf}: {info['rows']} –±–∞—Ä–æ–≤, {info['issues']} –ø—Ä–æ–±–ª–µ–º")
        
        logger.info("\n" + "="*70)
        logger.info("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        logger.info("="*70)
        
        logger.info("""
1. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ D1/H1 –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
2. ‚ö†Ô∏è M5 –¥–∞–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã 1 –≥–æ–¥–æ–º - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
3. üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –°–∫–∞—á–∞—Ç—å M1 –¥–∞–Ω–Ω—ã–µ –∏–∑ Dukascopy –∑–∞ 6 –ª–µ—Ç
4. üì¶ –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è: –∏—Å–ø–æ–ª—å–∑—É–π –≥–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Kaggle (M5 + H1 –∑–∞ 6 –ª–µ—Ç)
5. üîÑ –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ª—É—á—à–µ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è Dukascopy:
  npx dukascopy-node -i xauusd -from 2019-01-01 -to 2025-12-31 -t m1 -f csv --volumes

–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:
  https://www.kaggle.com/datasets/novandraanugrah/xauusd-gold-price-historical-data-2004-2024
        """)

if __name__ == '__main__':
    validator = DataValidator()
    validator.check_data_sources()
    validator.print_summary()
