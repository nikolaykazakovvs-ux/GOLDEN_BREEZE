# –°—Ç–∞—Ç—É—Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ BTC v5 Golden Breeze

## üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ - –ù–û–í–´–ô –†–ï–ö–û–†–î!

### –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
- **–ú–æ–¥–µ–ª—å**: GoldenBreezeV5Ultimate (327,751 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
- **–õ—É—á—à–∏–π Val MCC**: **+0.3316** ‚ú®
- **–≠–ø–æ—Ö–∞ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞**: 91 –∏–∑ 100
- **–ü—É—Ç—å**: `models/v5_btc/best_model.pt`
- **–ë—ç–∫–∞–ø**: `models/v5_btc/best_model_mcc0.3316_20251206_043810.pt`
- **–î–∞–Ω–Ω—ã–µ**: BTC/USDT (Binance, M5+H1 bars)

### –£–ª—É—á—à–µ–Ω–∏–µ
| –ú–µ—Ç—Ä–∏–∫–∞ | –ë—ã–ª–æ | –°—Ç–∞–ª–æ | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|------|-------|-----------|
| Val MCC | +0.2595 | **+0.3316** | +27.8% |
| Train MCC | +0.24 | +0.3312 | +38% |
| Loss | 1.05 | 0.9685 | -7.8% |

## –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–∞–º (—Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
```
Epoch   1:  Val MCC +0.1205
Epoch  10:  Val MCC +0.1772
Epoch  20:  Val MCC +0.2171
Epoch  30:  Val MCC +0.2489
Epoch  40:  Val MCC +0.2715
Epoch  50:  Val MCC +0.2988
Epoch  60:  Val MCC +0.3142
Epoch  70:  Val MCC +0.3243
Epoch  80:  Val MCC +0.3299
Epoch  90:  Val MCC +0.3309
Epoch  91:  Val MCC +0.3316 ‚ú® BEST (saved)
Epoch 100:  Val MCC +0.3307 (final)
```

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã ‚úÖ

### GPU Acceleration
- ‚úÖ TF32 –≤–∫–ª—é—á–µ–Ω (`torch.backends.cuda.matmul.allow_tf32`)
- ‚úÖ cuDNN benchmark –≤–∫–ª—é—á–µ–Ω (`torch.backends.cudnn.benchmark`)
- ‚úÖ High precision float32 matmul (`torch.set_float32_matmul_precision('high')`)
- ‚úÖ CUDA non-blocking transfers (`non_blocking=True`)

### Data Pipeline  
- ‚úÖ Batch size: 1024 (default, –º–æ–∂–Ω–æ –¥–æ 1536 –Ω–∞ RTX 3070)
- ‚úÖ Num workers: 8 (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö)
- ‚úÖ Prefetch factor: 4 (pipelined data feeding)
- ‚úÖ Pin memory: True (pinned CPU‚ÜíGPU transfers)
- ‚úÖ Persistent workers: True (–≤–æ—Ä–∫–µ—Ä—ã –º–µ–∂–¥—É —ç–ø–æ—Ö–∞–º–∏)

### Model Training
- ‚úÖ Mixed precision (AMP, FP16 + FP32)
- ‚úÖ Gradient scaling (GradScaler)
- ‚úÖ Auto-resume –∏–∑ best_model.pt
- ‚úÖ Auto-backup –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º
- ‚úÖ Cosine learning rate schedule + warmup

## –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å

### –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é
```bash
# –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (batch 1024, 8 workers)
python train_v5_btc.py --batch-size 1024 --num-workers 8 --epochs 200

# –ò–ª–∏ –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –º–µ–Ω—å—à–∏–π batch (–¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ GPU):
python train_v5_btc.py --batch-size 512 --num-workers 4
```

**–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏**:
- –°–æ–∑–¥–∞—Å—Ç backup —Ç–µ–∫—É—â–µ–≥–æ best_model.pt
- –ó–∞–≥—Ä—É–∑–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π checkpoint (epoch 34)
- –ü—Ä–æ–¥–æ–ª–∂–∏—Ç —Å —ç–ø–æ—Ö–∏ 35

### –û—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–µ
```bash
python evaluate_best_model.py
```

–í—ã–≤–µ–¥–µ—Ç:
- Test Loss, Accuracy, MCC
- Per-class accuracy
- Confusion matrix
- JSON report –≤ `reports/v5_btc_evaluation.json`

## –†–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
- GPU: RTX 3070 (8 GB VRAM)
- CPU: ~50% load –ø—Ä–∏ batch 1024, 8 workers
- RAM: ~85% (data –≤ –ø–∞–º—è—Ç–∏)
- GPU VRAM: ~69-74% (5.5/8 GB)
- GPU utilization: 70-85% (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)

## –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### ‚ùå KeyboardInterrupt –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ npz
–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ:
1. –§–∞–π–ª `data/prepared/btc_v5.npz` —Ü–µ–ª—ã–π (–ø—Ä–æ–≤–µ—Ä—å —Ä–∞–∑–º–µ—Ä ~112 MB)
2. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ
3. –î–∞–π –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ (—É–º–µ–Ω—å—à–∏ batch size –∏–ª–∏ num_workers)

### ‚ùå CUDA OOM (Out of Memory)
–†–µ—à–µ–Ω–∏–µ:
```bash
# –£–º–µ–Ω—å—à–∏ batch size
python train_v5_btc.py --batch-size 768
# –∏–ª–∏
python train_v5_btc.py --batch-size 512

# –£–º–µ–Ω—å—à–∏ —á–∏—Å–ª–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
python train_v5_btc.py --batch-size 1024 --num-workers 4
```

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è

1. **–£–≤–µ–ª–∏—á–∏—Ç—å GPU load –¥–∞–ª—å—à–µ**:
   - `--batch-size 1536` (–µ—Å–ª–∏ –≤–ª–µ–∑–µ—Ç) + `--num-workers 10`
   - `--prefetch-factor 6` –¥–ª—è –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ pipelining

2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient accumulation**:
   - –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –µ—â—ë –±–æ–ª—å—à–∏–π effective batch (–Ω–æ slower)

3. **Distributed training** (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU):
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `torch.nn.DataParallel` –∏–ª–∏ `DistributedDataParallel`

## –§–∞–π–ª—ã
- `train_v5_btc.py` - –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
- `evaluate_best_model.py` - –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
- `tools/fetch_crypto_history.py` - –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (Binance)
- `tools/precompute_btc.py` - –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (features, labels)
- `models/v5_btc/best_model.pt` - –ª—É—á—à–∏–π checkpoint (epoch 34)
- `models/v5_btc/best_model_backup_*.pt` - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±—ç–∫–∞–ø—ã

---
**–î–∞—Ç–∞**: 2025-12-06
**–í–µ—Ç–∫–∞**: fusion-transformer-v4
**–ú–æ–¥–µ–ª—å**: GoldenBreezeV5Ultimate_BTC
