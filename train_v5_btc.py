"""
Golden Breeze v5 BTC - Training Script

Trains a dedicated model for BTC/USDT crypto trading.
Based on v5 Ultimate architecture, optimized for crypto volatility.

Usage:
    python train_v5_btc.py --data-path data/prepared/btc_v5.npz

Author: Golden Breeze Team
Version: 5.1.0 BTC
Date: 2025-12-06
"""

import os
import sys
import json
import time
import shutil
import argparse
from pathlib import Path
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR
from torch.amp import autocast, GradScaler
from sklearn.metrics import matthews_corrcoef, accuracy_score, confusion_matrix
from tqdm import tqdm

# Add project root
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

from aimodule.models.v5_ultimate import GoldenBreezeV5Ultimate, V5UltimateConfig


# =============================================================================
# DATASET
# =============================================================================

class BTCDataset(Dataset):
    """
    Dataset for BTC v5 training.
    Works with pre-loaded arrays (no I/O per sample).
    """
    
    CLASS_NAMES = ['DOWN', 'NEUTRAL', 'UP']
    
    def __init__(self, x_fast, x_slow, x_strategy, labels_3class):
        """Initialize with already-loaded arrays."""
        # Convert to tensors immediately
        self.x_fast = torch.from_numpy(x_fast).float()
        self.x_slow = torch.from_numpy(x_slow).float()
        self.x_strategy = torch.from_numpy(x_strategy).float()
        self.labels = torch.from_numpy(labels_3class).long()
        
        self.n_samples = len(self.labels)
        
        print(f"   ‚úÖ Dataset created: {self.n_samples:,} samples")
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int):
        return (
            self.x_fast[idx],
            self.x_slow[idx],
            self.x_strategy[idx],
            self.labels[idx]
        )


# =============================================================================
# TRAINING UTILS
# =============================================================================

def get_warmup_scheduler(optimizer, warmup_steps: int, total_steps: int):
    """Linear warmup then cosine decay."""
    def lr_lambda(step):
        if step < warmup_steps:
            return float(step) / float(max(1, warmup_steps))
        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))
    return LambdaLR(optimizer, lr_lambda)


def train_epoch(model, loader, criterion, optimizer, scheduler, device, scaler, clip_grad=1.0):
    """Train one epoch."""
    model.train()
    
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    pbar = tqdm(loader, desc="Training", leave=False)
    for x_fast, x_slow, x_strat, labels in pbar:
        x_fast = x_fast.to(device, non_blocking=True)
        x_slow = x_slow.to(device, non_blocking=True)
        x_strat = x_strat.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        
        optimizer.zero_grad(set_to_none=True)
        
        # Mixed precision forward
        with autocast(device_type='cuda'):
            logits = model(x_fast, x_slow, x_strat)
            loss = criterion(logits, labels)
        
        # Backward with gradient scaling
        scaler.scale(loss).backward()
        
        # Optimizer step (–±–µ–∑ clipping - —ç—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏–µ)
        scaler.step(optimizer)
        scaler.update()
        
        # Scheduler step
        if scheduler is not None:
            scheduler.step()
        
        # Track
        total_loss += loss.item()
        preds = logits.argmax(dim=-1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
        
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    avg_loss = total_loss / len(loader)
    accuracy = accuracy_score(all_labels, all_preds)
    mcc = matthews_corrcoef(all_labels, all_preds)
    
    return {'loss': avg_loss, 'accuracy': accuracy, 'mcc': mcc}


@torch.no_grad()
def evaluate(model, loader, criterion, device):
    """Evaluate model."""
    model.eval()
    
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for x_fast, x_slow, x_strat, labels in tqdm(loader, desc="Evaluating", leave=False):
        x_fast = x_fast.to(device, non_blocking=True)
        x_slow = x_slow.to(device, non_blocking=True)
        x_strat = x_strat.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        
        with autocast(device_type='cuda'):
            logits = model(x_fast, x_slow, x_strat)
            loss = criterion(logits, labels)
        
        total_loss += loss.item()
        preds = logits.argmax(dim=-1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(loader)
    accuracy = accuracy_score(all_labels, all_preds)
    mcc = matthews_corrcoef(all_labels, all_preds)
    
    # Per-class accuracy
    cm = confusion_matrix(all_labels, all_preds)
    per_class = {}
    for i in range(3):
        if cm[i].sum() > 0:
            per_class[i] = cm[i, i] / cm[i].sum()
        else:
            per_class[i] = 0.0
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'mcc': mcc,
        'per_class': per_class,
        'confusion_matrix': cm,
    }


# =============================================================================
# MAIN TRAINING
# =============================================================================

def train(args):
    """Main training loop for BTC model."""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    os.makedirs(args.save_dir, exist_ok=True)

    # Backup existing best_model.pt to avoid losing progress on restart
    best_model_path = Path(args.save_dir) / 'best_model.pt'
    if best_model_path.exists():
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = Path(args.save_dir) / f"best_model_backup_{ts}.pt"
        shutil.copy2(best_model_path, backup_path)
        print(f"üì¶ Backup —Å–æ–∑–¥–∞–Ω: {backup_path}")

    if device == 'cuda':
        # –†–∞–∑—Ä–µ—à–∞–µ–º TF32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –Ω–∞ Ampere –∏ –≤–∫–ª—é—á–∞–µ–º autotune
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.set_float32_matmul_precision('high')
    
    print("=" * 70)
    print("ü™ô Golden Breeze v5 BTC Training")
    print("=" * 70)
    print(f"Device: {device}")
    print(f"Dataset: {args.data_path}")
    print(f"Batch size: {args.batch_size}")
    print(f"Epochs: {args.epochs}")
    print(f"Learning rate: {args.lr}")
    
    if device == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Load full dataset ONCE at start
    print("\nüìÇ Pre-loading full dataset into memory...")
    full_data = np.load(args.data_path, mmap_mode='r', allow_pickle=False)
    
    # Copy all arrays to RAM immediately
    print("   Reading x_fast...")
    x_fast_all = np.ascontiguousarray(full_data['x_fast'][:])
    print("   Reading x_slow...")
    x_slow_all = np.ascontiguousarray(full_data['x_slow'][:])
    print("   Reading x_strategy...")
    x_strategy_all = np.ascontiguousarray(full_data['x_strategy'][:])
    print("   Reading labels...")
    y_all = np.ascontiguousarray(full_data['y'][:])
    print("   ‚úÖ Full dataset loaded to RAM")
    
    # Create splits (70/15/15)
    np.random.seed(42)
    indices = np.random.permutation(len(y_all))
    
    train_size = int(0.70 * len(y_all))
    val_size = int(0.15 * len(y_all))
    
    train_idx = indices[:train_size]
    val_idx = indices[train_size:train_size + val_size]
    test_idx = indices[train_size + val_size:]
    
    print(f"   Train: {len(train_idx):,}")
    print(f"   Val: {len(val_idx):,}")
    print(f"   Test: {len(test_idx):,}")
    
    # Create datasets with pre-loaded data
    # Map 5-class to 3-class
    y_3class = np.where(
        y_all <= 1, 0,
        np.where(y_all == 2, 1, 2)
    )
    
    print("\nüìä Creating datasets...")
    
    train_ds = BTCDataset(
        x_fast_all[train_idx],
        x_slow_all[train_idx],
        x_strategy_all[train_idx],
        y_3class[train_idx]
    )
    
    val_ds = BTCDataset(
        x_fast_all[val_idx],
        x_slow_all[val_idx],
        x_strategy_all[val_idx],
        y_3class[val_idx]
    )
    
    test_ds = BTCDataset(
        x_fast_all[test_idx],
        x_slow_all[test_idx],
        x_strategy_all[test_idx],
        y_3class[test_idx]
    )
    
    # Get class weights from train set
    class_weights = torch.from_numpy(np.bincount(y_3class[train_idx]).astype(np.float32))
    class_weights = 1.0 / class_weights
    class_weights = class_weights / class_weights.sum() * len(class_weights)
    class_weights = class_weights.to(device)
    print(f"‚öñÔ∏è Class weights: {class_weights.tolist()}")
    
    # Create loaders
    loader_kwargs = {
        'batch_size': args.batch_size,
        'num_workers': args.num_workers,
        'pin_memory': True,
        'persistent_workers': args.num_workers > 0,
    }

    # Prefetch —É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–¥–∞—á—É –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –≤–æ—Ä–∫–µ—Ä–æ–≤
    if args.num_workers > 0:
        loader_kwargs['prefetch_factor'] = args.prefetch_factor
    
    train_loader = DataLoader(train_ds, shuffle=True, **loader_kwargs)
    val_loader = DataLoader(val_ds, shuffle=False, **loader_kwargs)
    test_loader = DataLoader(test_ds, shuffle=False, **loader_kwargs)
    
    # Create model
    config = V5UltimateConfig()
    model = GoldenBreezeV5Ultimate(config).to(device)
    
    total_params = model.count_parameters()
    print(f"\nüîß Model: GoldenBreezeV5Ultimate (BTC)")
    print(f"   Parameters: {total_params:,}")
    
    # Resume if checkpoint provided
    start_epoch = 1
    best_mcc = -1.0
    best_epoch = 0
    
    # Resume only if explicitly requested via --resume flag
    if args.resume:
        print(f"\nüìÇ Resuming from: {args.resume}")
        checkpoint = torch.load(args.resume, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        start_epoch = checkpoint.get('epoch', 0) + 1
        best_mcc = checkpoint.get('val_mcc', -1.0)
        best_epoch = checkpoint.get('epoch', 0)
        print(f"   Epoch {start_epoch}, best MCC: {best_mcc:+.4f}")
    
    # Optimizer, scheduler, loss
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    steps_per_epoch = len(train_loader)
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = args.warmup_epochs * steps_per_epoch
    
    scheduler = get_warmup_scheduler(optimizer, warmup_steps, total_steps)
    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)
    scaler = GradScaler()
    
    print(f"\nüìÖ Scheduler: Warmup ({args.warmup_epochs} epochs) + Cosine")
    print(f"   Total steps: {total_steps:,}")
    
    # Training loop
    patience_counter = 0
    
    print("\n" + "=" * 70)
    print("üöÄ Starting BTC Training")
    print("=" * 70)
    
    for epoch in range(start_epoch, args.epochs + 1):
        epoch_start = time.time()
        
        # Train
        train_metrics = train_epoch(
            model, train_loader, criterion, optimizer, scheduler,
            device, scaler, args.clip_grad
        )
        
        # Validate
        val_metrics = evaluate(model, val_loader, criterion, device)
        
        epoch_time = time.time() - epoch_start
        lr = optimizer.param_groups[0]['lr']
        
        print(f"Epoch {epoch:3d}/{args.epochs} | "
              f"Train Loss: {train_metrics['loss']:.4f} MCC: {train_metrics['mcc']:+.4f} | "
              f"Val Loss: {val_metrics['loss']:.4f} MCC: {val_metrics['mcc']:+.4f} | "
              f"LR: {lr:.2e} | {epoch_time:.1f}s")
        
        # Check improvement
        if val_metrics['mcc'] > best_mcc:
            best_mcc = val_metrics['mcc']
            best_epoch = epoch
            patience_counter = 0
            
            # Save best
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_mcc': best_mcc,
                'val_acc': val_metrics['accuracy'],
                'config': config.__dict__,
                'symbol': 'BTC/USDT',
            }
            torch.save(checkpoint, os.path.join(args.save_dir, 'best_model.pt'))
            print(f"   ‚ú® New best MCC: {best_mcc:+.4f} (saved)")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"\n‚ö†Ô∏è Early stopping at epoch {epoch}")
                break
        
        # Periodic checkpoint
        if epoch % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'val_mcc': val_metrics['mcc'],
            }, os.path.join(args.save_dir, f'checkpoint_epoch_{epoch}.pt'))
    
    # Final test
    print("\n" + "=" * 70)
    print("üìà Final BTC Test Evaluation")
    print("=" * 70)
    
    # Load best model
    checkpoint = torch.load(os.path.join(args.save_dir, 'best_model.pt'), weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    test_metrics = evaluate(model, test_loader, criterion, device)
    
    print(f"\nBTC Test Results:")
    print(f"   Loss: {test_metrics['loss']:.4f}")
    print(f"   Accuracy: {test_metrics['accuracy']:.4f}")
    print(f"   MCC: {test_metrics['mcc']:+.4f}")
    print(f"\nPer-Class Accuracy:")
    for i, name in enumerate(['DOWN', 'NEUTRAL', 'UP']):
        print(f"   {name}: {test_metrics['per_class'][i]:.4f}")
    print(f"\nConfusion Matrix:")
    print(test_metrics['confusion_matrix'])
    
    # Save report
    report = {
        'model': 'GoldenBreezeV5Ultimate_BTC',
        'symbol': 'BTC/USDT',
        'exchange': 'BINANCE',
        'date': datetime.now().isoformat(),
        'total_params': total_params,
        'best_epoch': best_epoch,
        'best_val_mcc': float(best_mcc),
        'test_metrics': {
            'loss': float(test_metrics['loss']),
            'accuracy': float(test_metrics['accuracy']),
            'mcc': float(test_metrics['mcc']),
            'per_class': {str(k): float(v) for k, v in test_metrics['per_class'].items()},
        },
        'args': vars(args),
    }
    
    with open(os.path.join(args.save_dir, 'training_report.json'), 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nü™ô BTC Training complete! Results saved to {args.save_dir}/")
    
    return best_mcc


def main():
    parser = argparse.ArgumentParser(description="Train v5 BTC Model")
    parser.add_argument('--data-path', type=str, default='data/prepared/btc_v5.npz')
    parser.add_argument('--save-dir', type=str, default='models/v5_btc')
    parser.add_argument('--batch-size', type=int, default=1024)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –±–∞—Ç—á –¥–ª—è –ª—É—á—à–µ–π –∑–∞–≥—Ä—É–∑–∫–∏ GPU
    parser.add_argument('--epochs', type=int, default=200)
    parser.add_argument('--patience', type=int, default=30)
    parser.add_argument('--lr', type=float, default=3e-4)
    parser.add_argument('--weight-decay', type=float, default=0.01)
    parser.add_argument('--warmup-epochs', type=int, default=5)
    parser.add_argument('--clip-grad', type=float, default=1.0)
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('--prefetch-factor', type=int, default=4, help='Prefetch batches per worker (>0 when num_workers>0)')
    parser.add_argument('--resume', type=str, default=None, help='Checkpoint to resume from (default: auto if best_model.pt exists)')
    
    args = parser.parse_args()
    train(args)


if __name__ == "__main__":
    main()
